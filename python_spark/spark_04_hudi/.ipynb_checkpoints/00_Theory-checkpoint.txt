https://www.youtube.com/watch?v=nGcT6RPjez4

Hudi: serverless transactional layer on top of data lakes
      multi-engine: decouple storage from compute, multiple engines like spark, flink..
      Copy-On-Read, Merge-on-Read: query performance, data freshness
      CDC on lakes


Copy-on-write table: queries snapshot, incremental
                     snapshot    - each commit produce a new snapshot of the data
                     incremental - each commit produce a new version of the files affected


Merge-on-read: queries snapshot, incremental, read-optimized
               updates and deleted will be lkogged to a delta log assisiated with each data file
               for workloads continusly flowed to the lake


How it works:

    Data are organized as file groups
    Each write is keys and mapped to one filegroup
    The version of the file will be updated in the file group or the logs will be writen in the file group
    After the write to the filegroup a new commit in commignlogs will be written

    The query will read the metadata table to get the latest point in time snapshot
    Then it will read the data files from the file groups


Reading the tables: 

    Read optimized query
    Snapshot query: get hte latest snapshot
    Incremental query: pull from log file to get the changes


Hudi as platform:

    Support both streaming and batch:
        As staate store for incremental merging results
        CDC event log like Kafka topics
    
    For data lake workloads:
        Large scale data processing


Hudi stack:

    Lake storage: cloud storage
            Wrapper filesystem: provide file size, buffering, metrics
            
    Open file format: 
            parquet, avro, orc

    Transactional database kernel:
        Concurency control
        Table services: file size, cleaning, archive
        Indexes:        bloom filters, hash indexes,
        Lake cache: columnar, mutable, hight available
        Table format: 
        Timeline server
    Programming APIs: 
        Writers: update, delete, log merge, hooks
        Readers: Snapshots, Timetravel, Change capture




    Platform services:
        Deltastreamer/ FlinkStreamer
        Deliver commit notifications
        Kafka Connect Sink
        Data quality checkers
        Snapshot, Restore, Export, Import

    Hudi Metaserver:
    Lake cache: 


Hudi delta streamer: https://www.youtube.com/watch?v=0ZskaSJlWVs

    1. need package: hudi utilities slim bundle
    2. use spark-submit to submit a spark job with prober configuariotns
        https://github.com/soumilshah1995/apache-hudi-delta-streamer-labs/blob/main/E1/Submit%20Spark%20Job