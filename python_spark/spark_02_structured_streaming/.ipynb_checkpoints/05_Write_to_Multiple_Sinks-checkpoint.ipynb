{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420b61b-1af1-4a0e-810e-842368efc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Each time the spark.writeStream is called the spark process the dag form spark.readStream\n",
    "    if i have two spark.writeStream then spark will process twice the stream\n",
    "    \n",
    "    The two write streams have seperate checkoint location and might have different offset because the one might process slower the messages \n",
    "    To solve this issues we use forEachBatch\n",
    "    forEachBatch microbatch we execute a python method\n",
    "    Inside the python method we can have as much spark.writeStreams we want\n",
    "    \n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "postgresql: create table device_data (customerId varchar(100),eventId varchar(100), eventOffset int,eventPublisher varchar(100),eventTime varchar(100),deviceId varchar(100),measure varchar(100),status varchar(100),temperature int);\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1eed63-86cb-40e4-97ff-3d6945bf18fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e6629c2ad701:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark write to two sinks</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f280c336110>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "    builder.\n",
    "    appName(\"Spark write to two sinks\").\n",
    "    config(\"spark.streaming.stopGracefullyOnShutdown\", True).\n",
    "    config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0').\n",
    "    config(\"spark.jars\", '/home/jovyan/jars/postgresql-42.2.20.jar').\n",
    "    config(\"spark.shuffle.partitions\", 4).\n",
    "    master(\"local[*]\").\n",
    "    getOrCreate()\n",
    "    \n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab65ce3-ad18-4edc-8b11-f0b9773602a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:19092\")\n",
    "    .option(\"subscribe\", \"device-data\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c38d626-ea05-422d-a48c-615d3c70a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "streaming_df = kafka_df.withColumn(\"value\", expr('cast(value as string)'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b91c495-38e1-4953-93e0-fc865adcf824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema of the Pyaload\n",
    "\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "json_schema = (\n",
    "    StructType(\n",
    "    [StructField('customerId', StringType(), True), \n",
    "    StructField('data', StructType(\n",
    "        [StructField('devices', \n",
    "                     ArrayType(StructType([ \n",
    "                        StructField('deviceId', StringType(), True), \n",
    "                        StructField('measure', StringType(), True), \n",
    "                        StructField('status', StringType(), True), \n",
    "                        StructField('temperature', LongType(), True)\n",
    "                    ]), True), True)\n",
    "        ]), True), \n",
    "    StructField('eventId', StringType(), True), \n",
    "    StructField('eventOffset', LongType(), True), \n",
    "    StructField('eventPublisher', StringType(), True), \n",
    "    StructField('eventTime', StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "streaming_df = streaming_df.withColumn(\"value_json\", from_json(\"value\", json_schema)).selectExpr(\"value_json.*\")\n",
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf43a9a2-fdc2-4c1f-ab53-28bf2d284dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "streaming_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))\n",
    "streaming_df_flatten = (\n",
    "    streaming_df.drop(\"data\")\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop(\"data_devices\")\n",
    ")\n",
    "\n",
    "streaming_df_flatten.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237e427-032c-46b2-afa8-52a279f6e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python function that will run for each batch\n",
    "def write_micro_batch(df, batch_id):\n",
    "    print(f\"Batch_di: {batch_id}\")\n",
    "\n",
    "    # write to parque\n",
    "    df.write.format(\"parquet\").mode(\"append\").save(\"/home/jovyan/data/multiplesinks.parquet/\")\n",
    "\n",
    "    # write to postgresql\n",
    "    (\n",
    "        df.\n",
    "        write.\n",
    "        format(\"jdbc\").\n",
    "        mode(\"append\").\n",
    "        option(\"driver\", \"org.postgresql.Driver\").\n",
    "        option(\"url\", \"jdbc:postgresql://source_postgresql:5432/source_pg\").\n",
    "        option(\"dbtable\", \"public.device_data\").\n",
    "        option(\"user\", \"root\").\n",
    "        option(\"password\", \"root\").\n",
    "        save()\n",
    "    )\n",
    "\n",
    "    df.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bc47b-f3d4-4a29-9bd4-eaf929ad90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    streaming_df_flatten.\n",
    "    writeStream.\n",
    "    foreachBatch(write_micro_batch).\n",
    "    trigger(processingTime='10 seconds').\n",
    "    option(\"checkpointLocation\", \"/home/jovyan/data/checkpoint_kafka\").\n",
    "    start().\n",
    "    awaitTermination()\n",
    ")\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da54ca2-c403-45b1-a822-6495d0f62aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
