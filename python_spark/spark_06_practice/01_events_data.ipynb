{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4521193-77cb-4014-b2f7-c572a2b9008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.session import get_spark_session\n",
    "\n",
    "spark = get_spark_session(\"01_events_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d31623b-8bae-4676-bcd3-4bf44b72b92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+------+\n",
      "|user_id|event_type|          timestamp|amount|\n",
      "+-------+----------+-------------------+------+\n",
      "|    101|  purchase|2025-08-14 10:00:00|   250|\n",
      "|    102|      view|2025-08-14 10:05:00|     0|\n",
      "|    101|  purchase|2025-08-14 11:00:00|   400|\n",
      "|    103|      view|2025-08-14 11:10:00|     0|\n",
      "|    102|  purchase|2025-08-14 11:30:00|   100|\n",
      "+-------+----------+-------------------+------+\n",
      "\n",
      "+-------+-------+\n",
      "|user_id|country|\n",
      "+-------+-------+\n",
      "|    101|    USA|\n",
      "|    102|     UK|\n",
      "|    103|Germany|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_evetns = spark.read.csv('./data/01_user_events.csv',header=True)\n",
    "df_evetns.show()\n",
    "\n",
    "df_users = spark.read.csv('./data/01_users.csv', header=True)\n",
    "df_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86148355-92c2-4cd4-898b-ead3c260d51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('user_id', StringType(), True), StructField('event_type', StringType(), True), StructField('timestamp', StringType(), True), StructField('amount', StringType(), True)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+------+\n",
      "|user_id|event_type|          timestamp|amount|\n",
      "+-------+----------+-------------------+------+\n",
      "|    101|  purchase|2025-08-14 10:00:00|   250|\n",
      "|    101|  purchase|2025-08-14 11:00:00|   400|\n",
      "|    102|  purchase|2025-08-14 11:30:00|   100|\n",
      "+-------+----------+-------------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('user_id', StringType(), True), StructField('event_type', StringType(), True), StructField('timestamp', StringType(), True), StructField('amount', StringType(), True)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|country|amount|\n",
      "+-------+------+\n",
      "|    USA| 250.0|\n",
      "|    USA| 400.0|\n",
      "|     UK| 100.0|\n",
      "+-------+------+\n",
      "\n",
      "+-------+-----+-----+\n",
      "|country|Total|  Avg|\n",
      "+-------+-----+-----+\n",
      "|    USA|650.0|325.0|\n",
      "|     UK|100.0|100.0|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Using PySpark, process the above data to produce a Parquet file showing the total purchase amount per country, sorted by amount in descending order. \n",
    "    Save the result to /output/purchases_by_country/ in overwrite mode.\n",
    "'''\n",
    "from pyspark.sql.functions import col, sum, avg\n",
    "\n",
    "# filter events of type purchase\n",
    "df_purchases = df_evetns.filter(\"event_type = 'purchase'\")\n",
    "display(df_purchases.schema)\n",
    "df_purchases.show()\n",
    "\n",
    "\n",
    "# join with users to get country\n",
    "df_purchase_country = df_purchases.join(df_users, df_purchases.user_id == df_users.user_id).select(\"country\", \"amount\").withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "display(df_purchases.schema)\n",
    "df_purchase_country.show()\n",
    "\n",
    "\n",
    "# group by country\n",
    "# df_purchase_country.groupBy(\"country\").agg({'amount': 'sum'}).withColumnRenamed(\"sum(amount)\", \"Total\").show()\n",
    "df_purchase_country.groupBy(\"country\").agg(sum(\"amount\").alias(\"Total\"), avg(\"amount\").alias(\"Avg\")).orderBy(col(\"Total\").desc()).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77ecbba6-f886-48af-973c-66f9277285df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Using PySpark, produce a DataFrame showing the number of events per country, regardless of event type.\n",
    "    Sort the result by event_count in descending order\n",
    "'''\n",
    "df_users.createOrReplaceTempView(\"users\")\n",
    "df_evetns.createOrReplaceTempView(\"events\")\n",
    "\n",
    "result = spark.sql(\"\"\"SELECT u.country, count(1) as event_count\n",
    "             FROM users AS u\n",
    "             INNER JOIN events AS e\n",
    "                ON u.user_id = e.user_id\n",
    "             GROUP BY u.country\n",
    "             ORDER BY event_count DESC\n",
    "          \"\"\")\n",
    "cols = ['country', 'event_count']\n",
    "data = [('USA', 2), ('UK', 2), ('Germany', 1)]\n",
    "expected = spark.createDataFrame(data, cols)\n",
    "\n",
    "\n",
    "actual = result.collect()\n",
    "expct = expected.collect()\n",
    "\n",
    "assert sorted(actual) == sorted(expct), f\"Data missmatch, result {actual}, excpected {expct}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec88b26-6fe3-4eba-ad97-69d4066c1388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
