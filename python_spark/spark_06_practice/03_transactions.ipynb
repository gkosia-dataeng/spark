{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9722caad-4693-4697-9aa4-ed434757017e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6ae36d1ddec3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>03_transactions_analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x71b0c6cbfcd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.session import get_spark_session\n",
    "\n",
    "\n",
    "spark  = get_spark_session(\"03_transactions_analysis\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c262ab81-e592-4aa6-a323-4eac477398b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------+\n",
      "|customer_id|   name|country|\n",
      "+-----------+-------+-------+\n",
      "|          1|  Alice|     US|\n",
      "|          2|    Bob|     UK|\n",
      "|          3|Charlie|     US|\n",
      "|          4|  Diana|     DE|\n",
      "+-----------+-------+-------+\n",
      "\n",
      "+--------------+-----------+------+-------------------+\n",
      "|transaction_id|customer_id|amount|          timestamp|\n",
      "+--------------+-----------+------+-------------------+\n",
      "|           101|          1|   250|2023-05-01 10:00:00|\n",
      "|           102|          1|   300|2023-05-03 12:00:00|\n",
      "|           103|          2|   450|2023-05-04 09:30:00|\n",
      "|           104|          3|   100|2023-05-05 14:00:00|\n",
      "|           105|          1|   500|2023-05-06 16:00:00|\n",
      "|           106|          4|   700|2023-05-07 18:00:00|\n",
      "+--------------+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_columns = [\"customer_id\",\"name\", \"country\"]\n",
    "cust_data = [\n",
    "    (1,\"Alice\",\"US\")\n",
    "    ,(2,\"Bob\"    ,\"UK\")\n",
    "    ,(3,\"Charlie\",\"US\")\n",
    "    ,(4,\"Diana\"  ,\"DE\")\n",
    "]\n",
    "\n",
    "df_customers = spark.createDataFrame(cust_data, cust_columns)\n",
    "df_customers.show()\n",
    "\n",
    "\n",
    "#tran_columns = \"transaction_id int, customer_id int, amount int, timestamp string\"\n",
    "tran_columns = [\"transaction_id\", \"customer_id\", \"amount\", \"timestamp\"]\n",
    "tran_data = [\n",
    "     (101,1,250,'2023-05-01 10:00:00')\n",
    "    ,(102,1,300,'2023-05-03 12:00:00')\n",
    "    ,(103,2,450,'2023-05-04 09:30:00')\n",
    "    ,(104,3,100,'2023-05-05 14:00:00')\n",
    "    ,(105,1,500,'2023-05-06 16:00:00')\n",
    "    ,(106,4,700,'2023-05-07 18:00:00')\n",
    "]\n",
    "\n",
    "df_transactions = spark.createDataFrame(tran_data, tran_columns)\n",
    "df_transactions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1417638-4c95-438f-b287-f39b414c7497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------+-------------------+-----------+\n",
      "|transaction_id|customer_id|amount|          timestamp|amount_rank|\n",
      "+--------------+-----------+------+-------------------+-----------+\n",
      "|           105|          1|   500|2023-05-06 16:00:00|          1|\n",
      "|           102|          1|   300|2023-05-03 12:00:00|          2|\n",
      "|           103|          2|   450|2023-05-04 09:30:00|          1|\n",
      "|           104|          3|   100|2023-05-05 14:00:00|          1|\n",
      "|           106|          4|   700|2023-05-07 18:00:00|          1|\n",
      "+--------------+-----------+------+-------------------+-----------+\n",
      "\n",
      "+-------+-----------+------+\n",
      "|country|customer_id|amount|\n",
      "+-------+-----------+------+\n",
      "|     US|          1|  1050|\n",
      "|     UK|          2|   450|\n",
      "|     US|          3|   100|\n",
      "|     DE|          4|   700|\n",
      "+-------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "   For each customer, find their top 2 highest transaction amounts.\n",
    "   Calculate total spend per customer and per country\n",
    "   If the transactions dataset is huge, how would you optimize the join?\n",
    "'''\n",
    "from pyspark.sql.functions import row_number, desc, sum, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy(\"customer_id\").orderBy(df_transactions[\"amount\"].desc())\n",
    "\n",
    "df_transactions_ranked = df_transactions.withColumn(\"amount_rank\", row_number().over(window)).where(\"amount_rank <= 2\")\n",
    "df_transactions_ranked.show()\n",
    "\n",
    "\n",
    "df_tran_by_user_country = (\n",
    "    df_transactions.alias('a').join(df_customers.alias('b'),col('a.customer_id') == col('b.customer_id')) \\\n",
    "    .groupBy(['country', 'a.customer_id']) \\\n",
    "    .agg(sum('amount').alias('amount'))\n",
    ")\n",
    "\n",
    "df_tran_by_user_country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "163bf35f-1a5c-4a49-a7b3-eb35ad35627e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------+-------------------+-------+-------+\n",
      "|customer_id|transaction_id|amount|          timestamp|   name|country|\n",
      "+-----------+--------------+------+-------------------+-------+-------+\n",
      "|          1|           101|   250|2023-05-01 10:00:00|  Alice|     US|\n",
      "|          1|           102|   300|2023-05-03 12:00:00|  Alice|     US|\n",
      "|          2|           103|   450|2023-05-04 09:30:00|    Bob|     UK|\n",
      "|          3|           104|   100|2023-05-05 14:00:00|Charlie|     US|\n",
      "|          1|           105|   500|2023-05-06 16:00:00|  Alice|     US|\n",
      "|          4|           106|   700|2023-05-07 18:00:00|  Diana|     DE|\n",
      "+-----------+--------------+------+-------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use broadcast join to avoid shuffle\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_customer_transactions = df_transactions.join(broadcast(df_customers), \"customer_id\")\n",
    "df_customer_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5dcccca-31ef-41bd-af0b-291d36acc43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------+-------------------+-------+-------+\n",
      "|customer_id|transaction_id|amount|          timestamp|   name|country|\n",
      "+-----------+--------------+------+-------------------+-------+-------+\n",
      "|          1|           101|   250|2023-05-01 10:00:00|  Alice|     US|\n",
      "|          1|           102|   300|2023-05-03 12:00:00|  Alice|     US|\n",
      "|          1|           105|   500|2023-05-06 16:00:00|  Alice|     US|\n",
      "|          2|           103|   450|2023-05-04 09:30:00|    Bob|     UK|\n",
      "|          3|           104|   100|2023-05-05 14:00:00|Charlie|     US|\n",
      "|          4|           106|   700|2023-05-07 18:00:00|  Diana|     DE|\n",
      "+-----------+--------------+------+-------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# repartition the data to optimize parralelism\n",
    "\n",
    "df_transactions = df_transactions.repartition(\"customer_id\")\n",
    "df_customers = df_customers.repartition(\"customer_id\")\n",
    "\n",
    "df_transactions.join(df_customers, \"customer_id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9900e8d-ed42-4176-bfe4-6a438f516852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
