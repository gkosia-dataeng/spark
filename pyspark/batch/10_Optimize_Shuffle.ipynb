{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d160c2aa-869b-43bc-b7c8-79dd97df2da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Pipelining\\n    \\n    Spark will try to compine as mutch posible narrow transfomrations in a single stage and apply them on a partition\\n    On a wide transformation spark will create a shuffle and a new stage\\n    On a shuffle the data are written on Shuffle files (Unsafe rows or Tungsten Binary Format) on disk and send to other executors over the network for the next stage\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Pipelining\n",
    "    \n",
    "    Spark will try to compine as mutch posible narrow transfomrations in a single stage and apply them on a partition\n",
    "    On a wide transformation spark will create a shuffle and a new stage\n",
    "    On a shuffle the data are written on Shuffle files (Unsafe rows or Tungsten Binary Format) on disk and send to other executors over the network for the next stage\n",
    "    \n",
    "    \n",
    "    Avoid shuffle operations if is possible\n",
    "    Repartition data properly\n",
    "    Filter data as earlier\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd462eb-8766-41fd-9233-8433b2ea980e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9a6c4d37ba39:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Reading from sockets</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f82e81a5900>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Reading from sockets\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\",4)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe48e52-ea1d-44cd-9793-6ff33414e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\",False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074e7043-c419-4fea-bc07-74de43622c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "emp = spark.read.format(\"csv\").schema(schema).option(\"header\", True).load(\"/home/jovyan/data/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1c7aaf-b511-41ec-8837-45f76a130bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Spark will create a job with two stages\n",
    "    The first stage has 16 tasks because i have 16 cores and will read the data\n",
    "    Then each task will write 10 records in shuffle files (1 row fir each department in each partition)\n",
    "    Then the second stage will have 200 tasks becasue of default.shuffle partitions but only 10 tasks will do job\n",
    "    \n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "amp_avg = emp.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\"))\n",
    "\n",
    "amp_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd86abb-3107-4d45-8b56-9579731f515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|part|count|\n",
      "+----+-----+\n",
      "|  12|65152|\n",
      "|   1|65190|\n",
      "|  13|65160|\n",
      "|   6|65176|\n",
      "|   3|65211|\n",
      "|   5|65238|\n",
      "|  15|22131|\n",
      "|   9|65172|\n",
      "|   4|65162|\n",
      "|   8|65212|\n",
      "|   7|65217|\n",
      "|  10|65206|\n",
      "|  11|65197|\n",
      "|  14|65150|\n",
      "|   2|65195|\n",
      "|   0|65231|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "emp.withColumn(\"part\", spark_partition_id()).groupBy(\"part\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "079e42fb-f8f3-4e41-b1b7-580c0f627947",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    By setting the spark.sql.shuffle.partitions to much lower will create much less tasks on shuffle and will be utilized better and faster\n",
    "'''\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",16)\n",
    "\n",
    "\n",
    "emp_avg = emp.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\"))\n",
    "\n",
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b17b5-3b44-4d7b-9175-41f3d4ffe9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
