{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35a80e76-c5e3-4b06-be76-5948867a6f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a34071f56a07:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Reading from sockets</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f30882966b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Reading from sockets\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.executor.memory\",\"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efdc479b-9b7b-4b04-84f3-5c210a181195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales CSV Data - 752MB Size ~ 7.2M Records\n",
    "\n",
    "_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/home/jovyan/data/new_sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd008250-e4ca-43dc-a189-8f14d07bdb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Every time we use the dataframe will scan the file and apply the transformations\n",
    "'''\n",
    "df = df.where(\"amount > 300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35957bdc-8f7b-41d8-a276-1608b022ce56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7202569"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    cache will be triggered when we need to run an action that will scan the file like write or count\n",
    "    In spark UI under storage tab i can see that the file is cached in  Disk Memory Deserialized storage level\n",
    "    Some of the data are stored in Memory and some on the disk\n",
    "'''\n",
    "\n",
    "df.cache().count() ## MEMORY_AND_DISK (DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525e2448-84cc-45fd-882e-b8932e2033fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "|       transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "|2017-11-24T19:00:...|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
      "|2017-11-24T19:00:...|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
      "|2017-11-24T19:00:...|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
      "|2017-11-24T19:00:...|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
      "|2017-11-24T19:00:...|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
      "|2017-11-24T19:00:...|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
      "|2017-11-24T19:00:...|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
      "|2017-11-24T19:00:...|2076946121|  562903918|unkn    ccd id: 5...| 315.86|1773943669|\n",
      "|2017-11-24T19:00:...|2076946063| 1070485878|Amazon.com   arc ...| 785.27|1126623009|\n",
      "|2017-11-24T19:00:...|2076944979| 1654681099|Delhaize America ...|  303.1|1243655802|\n",
      "|2017-11-24T19:00:...|2076944941| 1157343460|unkn    ppd id: 1...|2853.33|1141716004|\n",
      "|2017-11-24T19:00:...|2076944228| 1522061472|         YUM! Brands|1737.45| 592064091|\n",
      "|2017-11-24T19:00:...|2076944195| 1070485878|unkn   ppd id: 11...|2440.55|1525790470|\n",
      "|2017-11-24T19:00:...|2076944142|  847200066|Wal-Mart  ppd id:...| 331.63|1345953582|\n",
      "|2017-11-24T19:00:...|2076944073| 2077350195|Walgreen     arc ...|  396.9|2001708947|\n",
      "|2017-11-24T19:00:...|2076943052|  103953879|Rite Aid  arc id:...| 1910.8|1998549640|\n",
      "|2017-11-24T19:00:...|2076942340|  643354906|                BJ's|  372.7| 115209716|\n",
      "|2017-11-24T19:00:...|2076942282| 1445595477|Meijer    ccd id:...|  366.9|1717498102|\n",
      "|2017-11-24T19:00:...|2076942274| 2001148981|unkn    ppd id: 2...| 333.41| 559832710|\n",
      "|2017-11-24T19:00:...|2076942246|    9225731|AT&T Wireless  pp...|  396.9| 407629665|\n",
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    After caching the file when we use it, spark will read the data from memory and it will be much faster\n",
    "'''\n",
    "df.where(\"amount > 300\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a3f2ee3-fc4e-4723-98ae-c4f97656fffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[transacted_at: string, trx_id: string, retailer_id: string, description: string, amount: double, city_id: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove dataframe from cache\n",
    "df.unpersist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767c61fc-a919-4a62-a273-d15de6e9fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "|       transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "|2017-11-24T19:00:...|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
      "|2017-11-24T19:00:...|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
      "|2017-11-24T19:00:...|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
      "|2017-11-24T19:00:...|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
      "|2017-11-24T19:00:...|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
      "|2017-11-24T19:00:...|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
      "|2017-11-24T19:00:...|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
      "|2017-11-24T19:00:...|2076946121|  562903918|unkn    ccd id: 5...| 315.86|1773943669|\n",
      "|2017-11-24T19:00:...|2076946063| 1070485878|Amazon.com   arc ...| 785.27|1126623009|\n",
      "|2017-11-24T19:00:...|2076944979| 1654681099|Delhaize America ...|  303.1|1243655802|\n",
      "|2017-11-24T19:00:...|2076944941| 1157343460|unkn    ppd id: 1...|2853.33|1141716004|\n",
      "|2017-11-24T19:00:...|2076944228| 1522061472|         YUM! Brands|1737.45| 592064091|\n",
      "|2017-11-24T19:00:...|2076944195| 1070485878|unkn   ppd id: 11...|2440.55|1525790470|\n",
      "|2017-11-24T19:00:...|2076944142|  847200066|Wal-Mart  ppd id:...| 331.63|1345953582|\n",
      "|2017-11-24T19:00:...|2076944073| 2077350195|Walgreen     arc ...|  396.9|2001708947|\n",
      "|2017-11-24T19:00:...|2076943052|  103953879|Rite Aid  arc id:...| 1910.8|1998549640|\n",
      "|2017-11-24T19:00:...|2076942340|  643354906|                BJ's|  372.7| 115209716|\n",
      "|2017-11-24T19:00:...|2076942282| 1445595477|Meijer    ccd id:...|  366.9|1717498102|\n",
      "|2017-11-24T19:00:...|2076942274| 2001148981|unkn    ppd id: 2...| 333.41| 559832710|\n",
      "|2017-11-24T19:00:...|2076942246|    9225731|AT&T Wireless  pp...|  396.9| 407629665|\n",
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Spark keep liniage on dataframes so when we use df understand that the dataframe df_cached has the same data \n",
    "    It is use the cached data to answer df query\n",
    "    \n",
    "    Caching should be done to serve as much as queries \n",
    "    If cache only rows with amount > 300 (partial cache) and other query need other amount data it will scan the file\n",
    "'''\n",
    "\n",
    "df_cached = df.cache()\n",
    "df_cached.count()\n",
    "df.where(\"amount > 300\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f8c4b32-ccfa-4e1c-9e12-48f63ff28ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[transacted_at: string, trx_id: string, retailer_id: string, description: string, amount: double, city_id: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cached.unpersist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e98369f-914f-403b-a55b-c3bfd37fbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Cache storage level: with cache storage level is MEMORY_AND_DISK and data are desirialized\n",
    "                         with persist you can define the storage level\n",
    "                         MEMORY_ONLY, MEMORY_AND_DISK,MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK_2\n",
    "                         \n",
    "                         if data does not fit into memory and storage level is MEMORY_ONLY we will get error\n",
    "'''\n",
    "import pyspark\n",
    "\n",
    "df_persist = df.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "df_persist.write.format(\"noop\").mode(\"overwrite\").save() # write the dataframe to noop to trigger the cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21b5525e-7bef-4392-99fb-08b5d4282440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the cache\n",
    "spark.catalog.clearCache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
