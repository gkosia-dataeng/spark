{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5420b61b-1af1-4a0e-810e-842368efc805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    postgresql: create table device_data_error (key varchar(100), value varchar(max), eventtimestamp timestamp, batchid int);\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Error:\n",
    "        malformed data: payload is not in correct format or have missing fields\n",
    "\n",
    "        Each microbatch splited to two dataframes: correct records, malformed records\n",
    "        \n",
    "    Exceptions:\n",
    "        runtime error: db connection, network..\n",
    "\n",
    "        In case of exception we will write the whole batch in a location\n",
    "    \n",
    "    \n",
    "'''\n",
    "\n",
    "'''\n",
    "    postgresql: create table device_data_error (key varchar(100), value varchar(max), eventtimestamp timestamp, batchid int);\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1eed63-86cb-40e4-97ff-3d6945bf18fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e6629c2ad701:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Handle error and exceptions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f865d399490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "    builder.\n",
    "    appName(\"Handle error and exceptions\").\n",
    "    config(\"spark.streaming.stopGracefullyOnShutdown\", True).\n",
    "    config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0').\n",
    "    config(\"spark.jars\", '/home/jovyan/jars/postgresql-42.2.20.jar').\n",
    "    config(\"spark.shuffle.partitions\", 4).\n",
    "    master(\"local[*]\").\n",
    "    getOrCreate()\n",
    "    \n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab65ce3-ad18-4edc-8b11-f0b9773602a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:19092\")\n",
    "    .option(\"subscribe\", \"device-data\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67bc11c2-360b-4635-9979-852f60aa46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, lit, explode, col\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "from pyspark.sql.functions import from_json, current_timestamp\n",
    "\n",
    "def flatten_dataframe(df_message_kafka):\n",
    "\n",
    "    # convert binary message to string\n",
    "    streaming_df = df_message_kafka.withColumn(\"value\", expr('cast(value as string)'))\n",
    "\n",
    "    # define expected schema and parse message\n",
    "    json_schema = (\n",
    "        StructType(\n",
    "        [StructField('customerId', StringType(), True), \n",
    "        StructField('data', StructType(\n",
    "            [StructField('devices', \n",
    "                         ArrayType(StructType([ \n",
    "                            StructField('deviceId', StringType(), True), \n",
    "                            StructField('measure', StringType(), True), \n",
    "                            StructField('status', StringType(), True), \n",
    "                            StructField('temperature', LongType(), True)\n",
    "                        ]), True), True)\n",
    "            ]), True), \n",
    "        StructField('eventId', StringType(), True), \n",
    "        StructField('eventOffset', LongType(), True), \n",
    "        StructField('eventPublisher', StringType(), True), \n",
    "        StructField('eventTime', StringType(), True)\n",
    "        ])\n",
    "    )\n",
    "    streaming_df = streaming_df.withColumn(\"value_json\", from_json(\"value\", json_schema))\n",
    "\n",
    "    # filter error messages: not have customerid or device data\n",
    "    error_df = (\n",
    "                    streaming_df.\n",
    "                    select(\"key\", \"value\").\n",
    "                    withColumn(\"eventtimestamp\", lit(current_timestamp())).\n",
    "                    where(\"value_json.customerId is null or size(value_json.data.devices) == 0\")\n",
    "                )\n",
    "\n",
    "    # filter correct data\n",
    "    correct_df = (\n",
    "                streaming_df.\n",
    "                where(\"value_json.customerId is not null and size(value_json.data.devices) > 0\").\n",
    "                selectExpr(\"value_json.*\")\n",
    "            )\n",
    "    explode_device_data = correct_df.withColumn(\"data_devices\", explode(\"data.devices\"))\n",
    "    explode_device_data.show(truncate=False)\n",
    "    flatten_correct_data = (\n",
    "                                explode_device_data.\n",
    "                                drop(\"data\").\n",
    "                                withColumn(\"deviceId\", col(\"data_devices.deviceId\")).\n",
    "                                withColumn(\"measure\", col(\"data_devices.measure\")).\n",
    "                                withColumn(\"status\", col(\"data_devices.status\")).\n",
    "                                withColumn(\"temperature\", col(\"data_devices.temperature\")).\n",
    "                                drop(\"data_devices\")\n",
    "                            )\n",
    "    return flatten_correct_data, error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f37381-d9ee-4dd4-a828-7dee1e2012cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df_to_db(df, table_name):\n",
    "\n",
    "    # write to postgresql\n",
    "    (\n",
    "        df.\n",
    "        write.\n",
    "        format(\"jdbc\").\n",
    "        mode(\"append\").\n",
    "        option(\"driver\", \"org.postgresql.Driver\").\n",
    "        option(\"url\", \"jdbc:postgresql://source_postgresql:5432/source_pg\").\n",
    "        option(\"dbtable\", table_name).\n",
    "        option(\"user\", \"root\").\n",
    "        option(\"password\", \"root\").\n",
    "        save()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d237e427-032c-46b2-afa8-52a279f6e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python function that will run for each batch\n",
    "def write_micro_batch(df, batch_id):\n",
    "    print(f\"Batch_di: {batch_id}\")\n",
    "\n",
    "    try:\n",
    "        # split the new data to error and correct\n",
    "        correct_df_raw, error_df_raw = flatten_dataframe(df)\n",
    "    \n",
    "        # add batchid in error df\n",
    "        error_df_raw = error_df_raw.withColumn(\"batchid\", lit(batch_id))\n",
    "    \n",
    "        # wirte dataframes to db\n",
    "        write_df_to_db(correct_df_raw, \"public.device_data\")\n",
    "        write_df_to_db(error_df_raw, \"public.device_data_error\")\n",
    "        \n",
    "        correct_df_raw.show()\n",
    "        error_df_raw.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        df.write.format(\"parquet\").mode(\"append\").save(\"/home/jovyan/data/device_data_error.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7bc47b-f3d4-4a29-9bd4-eaf929ad90ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_di: 10\n",
      "+----------+--------------------------+------------------------------------+-----------+--------------+--------------------------+----------------------+\n",
      "|customerId|data                      |eventId                             |eventOffset|eventPublisher|eventTime                 |data_devices          |\n",
      "+----------+--------------------------+------------------------------------+-----------+--------------+--------------------------+----------------------+\n",
      "|CI00108   |{[{D004, C, SUCCESS, 16}]}|aa90011f-3967-496c-b94b-a0c8de19a3d3|10003      |device        |2023-01-05 11:13:53.643364|{D004, C, SUCCESS, 16}|\n",
      "+----------+--------------------------+------------------------------------+-----------+--------------+--------------------------+----------------------+\n",
      "\n",
      "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
      "|customerId|             eventId|eventOffset|eventPublisher|           eventTime|deviceId|measure| status|temperature|\n",
      "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
      "|   CI00108|aa90011f-3967-496...|      10003|        device|2023-01-05 11:13:...|    D004|      C|SUCCESS|         16|\n",
      "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
      "\n",
      "+---+-----+--------------+-------+\n",
      "|key|value|eventtimestamp|batchid|\n",
      "+---+-----+--------------+-------+\n",
      "+---+-----+--------------+-------+\n",
      "\n",
      "Batch_di: 11\n",
      "+----------+----+-------+-----------+--------------+---------+------------+\n",
      "|customerId|data|eventId|eventOffset|eventPublisher|eventTime|data_devices|\n",
      "+----------+----+-------+-----------+--------------+---------+------------+\n",
      "+----------+----+-------+-----------+--------------+---------+------------+\n",
      "\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "|customerId|eventId|eventOffset|eventPublisher|eventTime|deviceId|measure|status|temperature|\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "\n",
      "+----+--------------------+--------------------+-------+\n",
      "| key|               value|      eventtimestamp|batchid|\n",
      "+----+--------------------+--------------------+-------+\n",
      "|null|{\"eventId\": \"e885...|2024-04-10 10:56:...|     11|\n",
      "+----+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m (\n\u001b[1;32m      2\u001b[0m     \u001b[43mkafka_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriteStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeachBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_micro_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m10 seconds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/jovyan/data/checkpoint_kafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:201\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(\n",
    "    kafka_df.\n",
    "    writeStream.\n",
    "    foreachBatch(write_micro_batch).\n",
    "    trigger(processingTime='10 seconds').\n",
    "    option(\"checkpointLocation\", \"/home/jovyan/data/checkpoint_kafka\").\n",
    "    start().\n",
    "    awaitTermination()\n",
    ")\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da54ca2-c403-45b1-a822-6495d0f62aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
