{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604ff057-0b99-403d-9ea5-c5b7792f1e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8d3144cefb18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>challenge 0 - </code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4a4caddc10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.session import get_spark_session\n",
    "\n",
    "spark  = get_spark_session(\"challenge 0 - \")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b29a3c-66e0-41c9-9a5b-c66c200ed1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|   Name|Value|index|\n",
      "+-------+-----+-----+\n",
      "|  Alice|    1|    1|\n",
      "|    Bob|    2|    2|\n",
      "|Charlie|    3|    3|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create auto increment column\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "(\"Alice\", 1),\n",
    "(\"Bob\", 2),\n",
    "(\"Charlie\", 3),\n",
    "], [\"Name\", \"Value\"])\n",
    "\n",
    "\n",
    "wind = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "df.withColumn(\"index\", row_number().over(wind)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c65bd9e-1447-47f5-9b88-6802c1967a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   b|   2|\n",
      "|   c|   3|\n",
      "|   d|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lists to dataframe\n",
    "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "list2 = [1, 2, 3, 4]\n",
    "\n",
    "rdd_from_list = spark.sparkContext.parallelize(list(zip(list1, list2)))\n",
    "df_from_list = rdd_from_list.toDF([\"col1\", \"col2\"])\n",
    "df_from_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac5ff242-0da2-4b88-b349-9415308b21a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 2, 3, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# Get list of A does not exists in B\n",
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "\n",
    "ls_a = spark.sparkContext.parallelize(list_A)\n",
    "ls_b = spark.sparkContext.parallelize(list_B)\n",
    "\n",
    "'''\n",
    "    subtract: exists in A but not in A \n",
    "    union: all elements\n",
    "\n",
    "    collect: convert the rdd to list, or dataframe to Array[Row]  and return it to driver\n",
    "'''\n",
    "\n",
    "# in A not in B\n",
    "diff = ls_a.subtract(ls_b).collect()\n",
    "print(diff)\n",
    "\n",
    "\n",
    "# in A not in B or in B not in A\n",
    "not_in_B = ls_a.subtract(ls_b)\n",
    "not_in_A = ls_b.subtract(ls_a)\n",
    "\n",
    "\n",
    "print(not_in_B.union(not_in_A).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccacb639-d4b2-4cc8-a643-05388a87c90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.0, 20.0, 30.0, 50.0, 86.0]\n"
     ]
    }
   ],
   "source": [
    "# quantiles\n",
    "\n",
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# column, ntiles needed, error if 0 exact quantiles (expensive)\n",
    "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
    "\n",
    "print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c796e754-1a93-4c6f-9003-44e9b3679675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|name|freq|\n",
      "+----+----+\n",
      "|   a|   2|\n",
      "|   b|   2|\n",
      "|   c|   3|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# frequency\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "data = [\n",
    "    Row(name=\"a\", ),\n",
    "    Row(name=\"b\", ),\n",
    "    Row(name=\"b\", ),\n",
    "    Row(name=\"a\", ),\n",
    "    Row(name=\"c\", ),\n",
    "    Row(name=\"c\", ),\n",
    "    Row(name=\"c\", ),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.groupBy(\"name\").agg(count(\"name\").alias(\"freq\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d82f19b9-0173-47fa-b30a-6ddae64e7c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|    other|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keep only top 2 most frequent values\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import count, lit, col, when\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "\n",
    "most_2freq = df.groupBy('job').agg(count(lit(1)).alias('freq')).orderBy(col('freq').desc()).limit(2).select('job').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "df.withColumn('job', when(col('job').isin(most_2freq),col('job')).otherwise(lit('other'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0af103cc-7606-475c-8805-f2036dba4867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|null|\n",
      "|   B| null| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| null|null|\n",
      "+----+-----+----+\n",
      "\n",
      "+----+-----+---+\n",
      "|Name|Value| id|\n",
      "+----+-----+---+\n",
      "|   B| null|123|\n",
      "|   B|    3|456|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove rows with null in a column\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.dropna(subset=['id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebe3a0b3-2b6e-4db2-92f9-b11cfe9e41fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|new_col1|new_col2|new_col3|\n",
      "+--------+--------+--------+\n",
      "|       1|       2|       3|\n",
      "|       4|       5|       6|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename columns based on old new names lists\n",
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# old column names\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "# new column names\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "\n",
    "for old,new in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old,new)\n",
    "    \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55e0ca8e-ae36-47a0-83ca-d00b857c7c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stack two dataframes\n",
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "# Create DataFrame for region B\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "\n",
    "\n",
    "df_A.union(df_B).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d26ba291-4dc1-48f0-bb09-62e9dd324692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|length|\n",
      "+-----+------+\n",
      "| John|     4|\n",
      "|Alice|     5|\n",
      "|  Bob|     3|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert first to upper\n",
    "from pyspark.sql.functions import initcap, length\n",
    "\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.withColumn('name', initcap('name')).withColumn('length', length('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79779a69-413d-43ab-87e0-32e53e8453ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------------+-----------+--------------+\n",
      "|   name|age|salary|          id|prev_salary|diff_from_prev|\n",
      "+-------+---+------+------------+-----------+--------------+\n",
      "|  James| 34| 55000| 25769803776|       null|         55000|\n",
      "|Michael| 30| 70000| 51539607552|      55000|         15000|\n",
      "| Robert| 37| 60000| 77309411328|      70000|        -10000|\n",
      "|  Maria| 29| 80000|103079215104|      60000|         20000|\n",
      "|    Jen| 32| 65000|128849018880|      80000|        -15000|\n",
      "+-------+---+------+------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# difference with previous salary\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id, lag\n",
    "\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "w = Window.orderBy(\"id\")\n",
    "\n",
    "df.withColumn(\"prev_salary\", lag('salary').over(w)).withColumn(\"diff_from_prev\", col('salary') - when(col('prev_salary').isNull(), 0).otherwise(col('prev_salary'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b3385-2dfb-4944-8db9-575c2596816f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
