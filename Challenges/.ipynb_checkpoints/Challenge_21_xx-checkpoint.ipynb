{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49b13ce-c302-4ff0-9f30-fa7d560e32ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8d3144cefb18:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>challenge 0 - </code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f351b0d6750>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.session import get_spark_session\n",
    "\n",
    "spark  = get_spark_session(\"challenge 0 - \")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef06c8d5-67cd-48b0-8829-4dc1c12f54f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+----------+----------+\n",
      "|date_str_1| date_str_2|    date_1|    date_2|dayofmonth|\n",
      "+----------+-----------+----------+----------+----------+\n",
      "|2023-05-18|01 Jan 2010|2023-05-18|2010-01-01|         5|\n",
      "|2023-12-31|01 Jan 2010|2023-12-31|2010-01-01|         1|\n",
      "+----------+-----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, dayofmonth, weekofyear, dayofyear, dayofweek\n",
    "# get day of month, week number, day of year, day of week from date strings\n",
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df = df.withColumn(\"date_1\", to_date('date_str_1', 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"date_2\", to_date('date_str_2', 'dd MMM yyyy'))\n",
    "\n",
    "(\n",
    "    df.\n",
    "    withColumn(\"dayofmonth\", dayofmonth('date_1')).\n",
    "    withColumn(\"dayofmonth\", weekofyear('date_1')).\n",
    "    withColumn(\"dayofmonth\", dayofyear('date_1')).\n",
    "    withColumn(\"dayofmonth\", dayofweek('date_1')).\n",
    "    show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9354e8b-06bc-4bea-8d82-67d4f49762ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|MonthYear|    asDate|\n",
      "+---------+----------+\n",
      "| Jan 2010|2010-01-04|\n",
      "| Feb 2011|2011-02-04|\n",
      "| Mar 2012|2012-03-04|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert MMM yyyy to date as of 4th of month\n",
    "from pyspark.sql.functions import to_date, date_add\n",
    "\n",
    "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
    "\n",
    "df.withColumn('asDate', date_add(to_date('MonthYear', 'MMM yyyy'),3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bd1e905-a8b1-439f-8a99-47514df9da9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|rameses@egypt.com|\n",
      "|        matt@t.co|\n",
      "|narendra@modi.com|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter valid emails\n",
    "from pyspark.sql.functions import col\n",
    "data = ['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']\n",
    "\n",
    "# Convert the list to DataFrame\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "\n",
    "pattern = \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
    "\n",
    "df.where(col('value').rlike(pattern)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a01c02f0-abdf-4ec6-9c90-0ad0cd89d8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+----+\n",
      "|year|quarter|  EU|  US|\n",
      "+----+-------+----+----+\n",
      "|2021|      2|4500|5500|\n",
      "|2021|      1|4000|5000|\n",
      "|2021|      3|5000|6000|\n",
      "|2021|      4|6000|7000|\n",
      "+----+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pivot dataframe\n",
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "df_pivot = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum('revenue').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "012bd6ba-9dd6-4a1e-adf1-8af90621a70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|string           |new_string       |\n",
      "+-----------------+-----------------+\n",
      "|dbc deb abed gade|dbccdebcabedcgade|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace space with least frequent character in string\n",
    "df = spark.createDataFrame([('dbc deb abed gade',),], [\"string\"])\n",
    "\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def replace_least_freq_char(s):\n",
    "    counter = Counter(s.replace(\" \", \"\"))\n",
    "    least_freq = min(counter, key = counter.get)\n",
    "    return s.replace(\" \",least_freq)\n",
    "    \n",
    "\n",
    "func = udf(replace_least_freq_char, 'string')\n",
    "\n",
    "df.withColumn('new_string', func('string')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4c378-8dc7-4bfa-aeb8-4fa7e63274b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
