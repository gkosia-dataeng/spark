https://www.machinelearningplus.com/pyspark/pyspark-exercises-101-pyspark-exercises-for-data-analysis/#

Challenge_0_20: 
    2: auto increment column: window, monotonically_increasing_id
    3: lists to df: sparkContext.parallelize(list(zip))
    4,5: items in A not in B..: subtract, union
    6: percentile
    7: groupBy 
    8: .rdd.flatMap(lambda x: x).collect(), when().otherwise()
    9: df.dropna(subset=[])
    10: df.withColumnRenamed
    15: union
    17,19: initcap, length
    20: window, lag

Challenge_21_xx:
    21: to_date, dayofmonth, weekofyear, dayofyear, dayofweek
    22: date_add, to_date
    24: rlike
    25: pivot
    28: collections.Counter, udf
    29: sequence
    30: df.dtypes, df.columns
    32: iterate throw columns df.select(*(<expr> for c in df.columns))
    33: fillna with mean    agg, first, asDict
    35: format_number
    36: concat
    37: filter every nth row  monotonically_increasing_id, window
    38: get the id of the nth larger number 
    39: functools.reduce
    40: udf per row
    44: window, partitionBy, lag, lead
    45: count frequency of distinct values of dataframe
    46: dynamic list of select in select statement
    49: pivot a dataframe
    53: calculate mod (most frequent)
    55: convert column to lower using udf
    58: spark conf details
    69: calculate missing values ratio
    70: get all dataframe names created in env