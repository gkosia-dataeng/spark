{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49b13ce-c302-4ff0-9f30-fa7d560e32ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8d3144cefb18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>challenge 21 - </code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2328109710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.session import get_spark_session\n",
    "\n",
    "spark  = get_spark_session(\"challenge 21 - \")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef06c8d5-67cd-48b0-8829-4dc1c12f54f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+----------+----------+\n",
      "|date_str_1| date_str_2|    date_1|    date_2|dayofmonth|\n",
      "+----------+-----------+----------+----------+----------+\n",
      "|2023-05-18|01 Jan 2010|2023-05-18|2010-01-01|         5|\n",
      "|2023-12-31|01 Jan 2010|2023-12-31|2010-01-01|         1|\n",
      "+----------+-----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, dayofmonth, weekofyear, dayofyear, dayofweek\n",
    "# get day of month, week number, day of year, day of week from date strings\n",
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df = df.withColumn(\"date_1\", to_date('date_str_1', 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"date_2\", to_date('date_str_2', 'dd MMM yyyy'))\n",
    "\n",
    "(\n",
    "    df.\n",
    "    withColumn(\"dayofmonth\", dayofmonth('date_1')).\n",
    "    withColumn(\"dayofmonth\", weekofyear('date_1')).\n",
    "    withColumn(\"dayofmonth\", dayofyear('date_1')).\n",
    "    withColumn(\"dayofmonth\", dayofweek('date_1')).\n",
    "    show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9354e8b-06bc-4bea-8d82-67d4f49762ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|MonthYear|    asDate|\n",
      "+---------+----------+\n",
      "| Jan 2010|2010-01-04|\n",
      "| Feb 2011|2011-02-04|\n",
      "| Mar 2012|2012-03-04|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert MMM yyyy to date as of 4th of month\n",
    "from pyspark.sql.functions import to_date, date_add\n",
    "\n",
    "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
    "\n",
    "df.withColumn('asDate', date_add(to_date('MonthYear', 'MMM yyyy'),3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bd1e905-a8b1-439f-8a99-47514df9da9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|rameses@egypt.com|\n",
      "|        matt@t.co|\n",
      "|narendra@modi.com|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter valid emails\n",
    "from pyspark.sql.functions import col\n",
    "data = ['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']\n",
    "\n",
    "# Convert the list to DataFrame\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "\n",
    "pattern = \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
    "\n",
    "df.where(col('value').rlike(pattern)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a01c02f0-abdf-4ec6-9c90-0ad0cd89d8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+----+\n",
      "|year|quarter|  EU|  US|\n",
      "+----+-------+----+----+\n",
      "|2021|      2|4500|5500|\n",
      "|2021|      1|4000|5000|\n",
      "|2021|      3|5000|6000|\n",
      "|2021|      4|6000|7000|\n",
      "+----+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pivot dataframe\n",
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "df_pivot = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum('revenue').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "012bd6ba-9dd6-4a1e-adf1-8af90621a70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|string           |new_string       |\n",
      "+-----------------+-----------------+\n",
      "|dbc deb abed gade|dbccdebcabedcgade|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace space with least frequent character in string\n",
    "df = spark.createDataFrame([('dbc deb abed gade',),], [\"string\"])\n",
    "\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def replace_least_freq_char(s):\n",
    "    counter = Counter(s.replace(\" \", \"\"))\n",
    "    least_freq = min(counter, key = counter.get)\n",
    "    return s.replace(\" \",least_freq)\n",
    "    \n",
    "\n",
    "func = udf(replace_least_freq_char, 'string')\n",
    "\n",
    "df.withColumn('new_string', func('string')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10b4c378-8dc7-4bfa-aeb8-4fa7e63274b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|       day|randint|\n",
      "+----------+-------+\n",
      "|2000-01-01|      5|\n",
      "|2000-01-08|      0|\n",
      "|2000-01-15|      6|\n",
      "|2000-01-22|      6|\n",
      "|2000-01-29|      1|\n",
      "|2000-02-05|      3|\n",
      "|2000-02-12|      2|\n",
      "|2000-02-19|      7|\n",
      "|2000-02-26|      0|\n",
      "|2000-03-04|      4|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, explode, sequence, rand\n",
    "\n",
    "#generate a df with saturndays dates and random numbers\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2000-3-04\"\n",
    "\n",
    "df = (\n",
    "    spark.\n",
    "    range(1).\n",
    "    select(\n",
    "        sequence(\n",
    "            expr(f\"date '{start_date}'\"),\n",
    "            expr(f\"date '{end_date}'\"),\n",
    "            expr(\"interval 1 day\")\n",
    "        ).alias(\"dates\")\n",
    "    ).\n",
    "    withColumn(\"day\", explode(\"dates\")).\n",
    "    where(\"dayofweek(day) = 7\").\n",
    "    withColumn(\"randint\", (rand(seed=42) * 10).cast('int')).\n",
    "    select(\"day\", \"randint\")\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6943feb7-f13a-42a4-ab23-20e11ae46933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('day', 'date'), ('randint', 'int')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['day', 'randint']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efd82464-7163-4859-a8fb-99dcc7f08b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "| name|age|qty|\n",
      "+-----+---+---+\n",
      "|Alice|  1| 30|\n",
      "|  Bob|  2| 35|\n",
      "+-----+---+---+\n",
      "\n",
      "+-----+--------+--------+\n",
      "| name|user_age|user_qty|\n",
      "+-----+--------+--------+\n",
      "|Alice|       1|      30|\n",
      "|  Bob|       2|      35|\n",
      "+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have the following DataFrame\n",
    "df = spark.createDataFrame([('Alice', 1, 30),('Bob', 2, 35)], [\"name\", \"age\", \"qty\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Rename lists for specific columns\n",
    "old_names = [\"qty\", \"age\"]\n",
    "new_names = [\"user_qty\", \"user_age\"]\n",
    "\n",
    "for o,n in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(o,n)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51adedc4-c0e0-41c1-ae7d-c08a7897fd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|Name|Value| id|\n",
      "+----+-----+---+\n",
      "|   0|    2|  2|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, col\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "\n",
    "df.select(*( sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1768ac17-0b0f-4546-ac04-9473fba10a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Name|var1|var2|\n",
      "+----+----+----+\n",
      "|   A|   1|null|\n",
      "|   B|null| 123|\n",
      "|   B|   3| 456|\n",
      "|   D|   6|null|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+\n",
      "|Name|var1|var2|\n",
      "+----+----+----+\n",
      "|   A|   1| 289|\n",
      "|   B|   3| 123|\n",
      "|   B|   3| 456|\n",
      "|   D|   6| 289|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, 123 ),\n",
    "(\"B\", 3, 456),\n",
    "(\"D\", 6, None),\n",
    "], [\"Name\", \"var1\", \"var2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "(\n",
    "    df.\n",
    "    fillna(\n",
    "            df.\n",
    "            agg(*( mean(col(c)).alias(c) for c in df.columns if c in [\"var1\", \"var2\"])).\n",
    "            first().\n",
    "            asDict()\n",
    "           )\n",
    ").show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbe109d6-88ed-4e94-950b-3fe493fbfd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|your_column|\n",
      "+---+-----------+\n",
      "|  1|    1.23E-7|\n",
      "|  2|  2.3456E-5|\n",
      "|  3| 3.45678E-4|\n",
      "+---+-----------+\n",
      "\n",
      "+---+------------+\n",
      "| id| your_column|\n",
      "+---+------------+\n",
      "|  1|0.0000001230|\n",
      "|  2|0.0000234560|\n",
      "|  3|0.0003456780|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "df = spark.createDataFrame([(1, 0.000000123), (2, 0.000023456), (3, 0.000345678)], [\"id\", \"your_column\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.withColumn(\"your_column\", format_number(\"your_column\",10)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f13ffeb3-f78c-4a8f-8101-88c58f358789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|numbers_1|numbers_2|\n",
      "+---------+---------+\n",
      "|      0.1|     0.08|\n",
      "|      0.2|     0.06|\n",
      "|     0.33|     0.02|\n",
      "+---------+---------+\n",
      "\n",
      "+---------+---------+\n",
      "|numbers_1|numbers_2|\n",
      "+---------+---------+\n",
      "|   10.00%|    8.00%|\n",
      "|   20.00%|    6.00%|\n",
      "|   33.00%|    2.00%|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, concat\n",
    "data = [(0.1, .08), (0.2, .06), (0.33, .02)]\n",
    "df = spark.createDataFrame(data, [\"numbers_1\", \"numbers_2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "df.select(*(concat((col(c) * lit(100)).cast('decimal(10,2)'), lit(\"%\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20c00d02-4cb7-493d-9125-ea21e174e2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   Name|Number|\n",
      "+-------+------+\n",
      "|  Alice|     1|\n",
      "|    Bob|     2|\n",
      "|Charlie|     3|\n",
      "|   Dave|     4|\n",
      "|    Eve|     5|\n",
      "|  Frank|     6|\n",
      "|  Grace|     7|\n",
      "| Hannah|     8|\n",
      "|   Igor|     9|\n",
      "|   Jack|    10|\n",
      "+-------+------+\n",
      "\n",
      "+----+------+---+\n",
      "|Name|Number| id|\n",
      "+----+------+---+\n",
      "| Eve|     5|  5|\n",
      "|Jack|    10| 10|\n",
      "+----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3), (\"Dave\", 4), (\"Eve\", 5),\n",
    "(\"Frank\", 6), (\"Grace\", 7), (\"Hannah\", 8), (\"Igor\", 9), (\"Jack\", 10)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Number\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "(\n",
    "    df.\n",
    "    withColumn(\"id\", row_number().over(w)).\n",
    "    where(\"id % 5 == 0\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ab11f7b-eb62-49bf-961a-ef76ee9ca0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|column1|\n",
      "+---+-------+\n",
      "|  1|      5|\n",
      "|  2|      8|\n",
      "|  3|     12|\n",
      "|  4|      1|\n",
      "|  5|     15|\n",
      "|  6|      7|\n",
      "+---+-------+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "Row(id=1, column1=5),\n",
    "Row(id=2, column1=8),\n",
    "Row(id=3, column1=12),\n",
    "Row(id=4, column1=1),\n",
    "Row(id=5, column1=15),\n",
    "Row(id=6, column1=7),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n",
    "w = Window().orderBy(col(\"column1\").desc())\n",
    "\n",
    "df.withColumn(\"ord\", row_number().over(w)).where(\"ord = 1\").select(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "087e4490-ce26-4172-9451-48a4b7093787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|  10|  25|  70|\n",
      "|  40|   5|  20|\n",
      "|  70|  80| 100|\n",
      "|  10|   2|  60|\n",
      "|  40|  50|  20|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+-------+\n",
      "|col1|col2|col3|row_sum|\n",
      "+----+----+----+-------+\n",
      "|  10|  25|  70|    105|\n",
      "|  40|   5|  20|     65|\n",
      "|  70|  80| 100|    250|\n",
      "|  10|   2|  60|     72|\n",
      "|  40|  50|  20|    110|\n",
      "+----+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# Sample data\n",
    "data = [(10, 25, 70),\n",
    "(40, 5, 20),\n",
    "(70, 80, 100),\n",
    "(10, 2, 60),\n",
    "(40, 50, 20)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# Display original DataFrame\n",
    "df.show()\n",
    "\n",
    "df.withColumn('row_sum', reduce(lambda a,b: a+b, [F.col(c) for c in df.columns])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "87792410-2e7f-4b8b-a4fd-772425e713ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "|   7|   8|   9|\n",
      "|  10|  11|  12|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+----+----------+\n",
      "|col1|col2|col3|min_by_max|\n",
      "+----+----+----+----------+\n",
      "|   1|   2|   3|0.33333334|\n",
      "|   4|   5|   6| 0.6666667|\n",
      "|   7|   8|   9| 0.7777778|\n",
      "|  10|  11|  12| 0.8333333|\n",
      "+----+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, array\n",
    "from pyspark.sql.types import FloatType\n",
    "# Sample Data\n",
    "data = [(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n",
    "def find_min_by_max(row):\n",
    "    return min(row)/max(row)\n",
    "    \n",
    "\n",
    "min_by_max_func = udf(find_min_by_max, FloatType())\n",
    "\n",
    "df.withColumn(\"min_by_max\", min_by_max_func(array(df.columns))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cf57163b-28dd-42c6-b64e-fb7d225b07b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|      Date| Store|Sales|\n",
      "+----------+------+-----+\n",
      "|2023-01-01|Store1|  100|\n",
      "|2023-01-02|Store1|  150|\n",
      "|2023-01-03|Store1|  200|\n",
      "|2023-01-04|Store1|  250|\n",
      "|2023-01-05|Store1|  300|\n",
      "|2023-01-01|Store2|   50|\n",
      "|2023-01-02|Store2|   60|\n",
      "|2023-01-03|Store2|   80|\n",
      "|2023-01-04|Store2|   90|\n",
      "|2023-01-05|Store2|  120|\n",
      "+----------+------+-----+\n",
      "\n",
      "+----------+------+-----+----+----+\n",
      "|      Date| Store|Sales| lag|lead|\n",
      "+----------+------+-----+----+----+\n",
      "|2023-01-01|Store1|  100|null| 150|\n",
      "|2023-01-02|Store1|  150| 100| 200|\n",
      "|2023-01-03|Store1|  200| 150| 250|\n",
      "|2023-01-04|Store1|  250| 200| 300|\n",
      "|2023-01-05|Store1|  300| 250|null|\n",
      "|2023-01-01|Store2|   50|null|  60|\n",
      "|2023-01-02|Store2|   60|  50|  80|\n",
      "|2023-01-03|Store2|   80|  60|  90|\n",
      "|2023-01-04|Store2|   90|  80| 120|\n",
      "|2023-01-05|Store2|  120|  90|null|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"2023-01-01\", \"Store1\", 100),\n",
    "(\"2023-01-02\", \"Store1\", 150),\n",
    "(\"2023-01-03\", \"Store1\", 200),\n",
    "(\"2023-01-04\", \"Store1\", 250),\n",
    "(\"2023-01-05\", \"Store1\", 300),\n",
    "(\"2023-01-01\", \"Store2\", 50),\n",
    "(\"2023-01-02\", \"Store2\", 60),\n",
    "(\"2023-01-03\", \"Store2\", 80),\n",
    "(\"2023-01-04\", \"Store2\", 90),\n",
    "(\"2023-01-05\", \"Store2\", 120)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Date\", \"Store\", \"Sales\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, lead\n",
    "\n",
    "w = Window().partitionBy(\"Store\").orderBy(\"Date\")\n",
    "\n",
    "(\n",
    "    df.\n",
    "    withColumn(\"lag\", lag(\"Sales\").over(w)).\n",
    "    withColumn(\"lead\", lead(\"Sales\").over(w))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "14e5df80-10b2-4277-9ae4-4fcc64e331e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|Column1|Column2|Column3|\n",
      "+-------+-------+-------+\n",
      "|      1|      2|      3|\n",
      "|      2|      3|      4|\n",
      "|      1|      2|      3|\n",
      "|      4|      5|      6|\n",
      "|      2|      3|      4|\n",
      "+-------+-------+-------+\n",
      "\n",
      "+-------+--------+\n",
      "|Column1|count(1)|\n",
      "+-------+--------+\n",
      "|      1|       2|\n",
      "|      2|       4|\n",
      "|      4|       3|\n",
      "|      3|       4|\n",
      "|      5|       1|\n",
      "|      6|       1|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(1, 2, 3),\n",
    "(4, 5, 6),\n",
    "(2, 3, 4)]\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "\n",
    "df.select(\"Column1\").unionAll(df.select(\"Column2\")).unionAll(df.select(\"Column3\")).groupBy(\"Column1\").agg(count(lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2fb104-7318-443e-9cab-01400d848cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|col_1|col_2|col_3|col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    1|    2|    3|    4|\n",
      "|    2|    3|    4|    5|\n",
      "|    1|    2|    3|    4|\n",
      "|    4|    5|    6|    7|\n",
      "+-----+-----+-----+-----+\n",
      "\n",
      "+-----+-----+-----+-----+\n",
      "|col_1|col_2|col_3|col_4|\n",
      "+-----+-----+-----+-----+\n",
      "|    0|    2|    3|    4|\n",
      "|    2|    0|    4|    5|\n",
      "|    1|    2|    0|    4|\n",
      "|    4|    5|    6|    0|\n",
      "+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(1, 2, 3, 4),\n",
    "(4, 5, 6, 7)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "(\n",
    "    df.\n",
    "    withColumn(\"row_num\", row_number().over(w) - 1).\n",
    "    select([when(col(\"row_num\") == i,0).otherwise(col(\"col_\" + str(i+1))).alias(\"col_\" + str(i+1)) for i in range(4)])\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e2ad3-8b44-4d30-b623-22fef7cbce64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
